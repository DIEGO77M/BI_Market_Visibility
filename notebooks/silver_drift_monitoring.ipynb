{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5ac563",
   "metadata": {},
   "source": [
    "# Silver Drift Monitoring Notebook\n",
    "\n",
    "This notebook implements lightweight drift monitoring for the Silver layer in a Databricks Medallion architecture. It is designed for metadata-only checks (schema, quality, and volume drift) and logs results to an audit table, following best practices for Databricks Serverless and Delta Lake.\n",
    "\n",
    "**Author:** Senior Data Architect\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Structure\n",
    "- **1. Imports and Setup**\n",
    "- **2. Utility Functions**\n",
    "- **3. Drift Monitoring Logic**\n",
    "- **4. Main Execution**\n",
    "- **5. Results and Audit Table**\n",
    "\n",
    "> _All code and explanations are in English for professional portfolio use. Please refer to the README and documentation for further details._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c9943",
   "metadata": {},
   "source": [
    "## 1. Imports and Spark Session\n",
    "\n",
    "This section imports the required libraries and initializes the Spark session for Databricks Connect. All code is optimized for Serverless and Delta Lake best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b9e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, countDistinct\n",
    "from delta.tables import DeltaTable\n",
    "import datetime\n",
    "\n",
    "# Initialize Spark session (Databricks Connect)\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cabc21",
   "metadata": {},
   "source": [
    "## 2. Utility Functions\n",
    "\n",
    "This section defines helper functions for schema extraction, data quality checks, and audit table management. All logic is modular and reusable for professional data engineering workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ffcc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"default\"\n",
    "SILVER_TABLES = [\n",
    "    \"silver_master_pdv\",\n",
    "    \"silver_master_products\",\n",
    "    \"silver_price_audit\",\n",
    "    \"silver_sell_in\"\n",
    "]\n",
    "AUDIT_TABLE = f\"{CATALOG}.{SCHEMA}.silver_drift_history\"\n",
    "\n",
    "# --- Utility: Get current schema as dict ---\n",
    "def get_table_schema(spark, table):\n",
    "    schema = spark.table(table).schema\n",
    "    return {f.name: str(f.dataType) for f in schema.fields}\n",
    "\n",
    "# --- Utility: Get Delta History metrics ---\n",
    "def get_delta_history_metrics(spark, table):\n",
    "    hist = spark.sql(f\"DESCRIBE HISTORY {table} LIMIT 1\").first()\n",
    "    metrics = hist[\"operationMetrics\"]\n",
    "    return {\n",
    "        \"numOutputRows\": int(metrics.get(\"numOutputRows\", 0)),\n",
    "        \"numFiles\": int(metrics.get(\"numFiles\", 0)),\n",
    "        \"timestamp\": hist[\"timestamp\"]\n",
    "    }\n",
    "\n",
    "# --- Utility: Load baseline (from audit table or static) ---\n",
    "def load_baseline(spark, table):\n",
    "    import json\n",
    "    try:\n",
    "        # Check if audit table exists (serverless-friendly)\n",
    "        if not spark._jsparkSession.catalog().tableExists(AUDIT_TABLE):\n",
    "            return None\n",
    "        df = spark.table(AUDIT_TABLE).filter(col(\"table_name\") == table).orderBy(col(\"timestamp\").desc())\n",
    "        last = df.first()\n",
    "        if last:\n",
    "            return json.loads(last[\"baseline_json\"])\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbcdf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility: Save drift event to audit table ---\n",
    "def save_drift_event(spark, table, drift_type, severity, details, baseline_json):\n",
    "    from pyspark.sql import Row\n",
    "    import json\n",
    "    event = Row(\n",
    "        table_name=table,\n",
    "        drift_type=drift_type,\n",
    "        severity=severity,\n",
    "        details=json.dumps(details),\n",
    "        baseline_json=json.dumps(baseline_json),\n",
    "        timestamp=datetime.datetime.now()\n",
    "    )\n",
    "    df = spark.createDataFrame([event])\n",
    "    df.write.format(\"delta\").mode(\"append\").saveAsTable(AUDIT_TABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88754e5e",
   "metadata": {},
   "source": [
    "## 3. Drift Monitoring Logic\n",
    "\n",
    "This section implements the main drift detection logic for schema, quality, and volume drift. Each function is designed for metadata-only checks and logs results to the audit table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf280af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility: Compare schemas ---\n",
    "def compare_schema(current, baseline):\n",
    "    drift = {\"new_columns\": [], \"missing_columns\": [], \"type_changes\": []}\n",
    "    if not baseline:\n",
    "        return drift\n",
    "    for col in current:\n",
    "        if col not in baseline:\n",
    "            drift[\"new_columns\"].append(col)\n",
    "        elif current[col] != baseline[col]:\n",
    "            drift[\"type_changes\"].append({\"column\": col, \"old\": baseline[col], \"new\": current[col]})\n",
    "    for col in baseline:\n",
    "        if col not in current:\n",
    "            drift[\"missing_columns\"].append(col)\n",
    "    return drift\n",
    "\n",
    "# --- Utility: Compare metrics ---\n",
    "def compare_metrics(current, baseline, key=\"numOutputRows\", threshold=0.2):\n",
    "    if not baseline or key not in baseline or key not in current:\n",
    "        return None\n",
    "    prev = baseline[key]\n",
    "    curr = current[key]\n",
    "    if prev == 0:\n",
    "        return None\n",
    "    change = (curr - prev) / prev\n",
    "    if abs(change) > threshold:\n",
    "        return {\"metric\": key, \"prev\": prev, \"curr\": curr, \"change\": change}\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main monitoring loop ---\n",
    "def run_silver_drift_monitoring(spark):\n",
    "    for table in SILVER_TABLES:\n",
    "        full_table = f\"{CATALOG}.{SCHEMA}.{table}\"\n",
    "        # 1. Get current schema and metrics\n",
    "        schema_now = get_table_schema(spark, full_table)\n",
    "        metrics_now = get_delta_history_metrics(spark, full_table)\n",
    "        # 2. Load baseline\n",
    "        baseline = load_baseline(spark, table)\n",
    "        # 3. Compare schema\n",
    "        schema_drift = compare_schema(schema_now, baseline[\"schema\"] if baseline else None)\n",
    "        # 4. Compare volume\n",
    "        volume_drift = compare_metrics(metrics_now, baseline[\"metrics\"] if baseline else None)\n",
    "        # 5. Quality drift: use Silver logs if available (placeholder)\n",
    "        # (You can extend this to read from a Silver quality log table)\n",
    "        # 6. Severity logic\n",
    "        severity = \"LOW\"\n",
    "        details = []\n",
    "        if schema_drift[\"missing_columns\"] or schema_drift[\"type_changes\"]:\n",
    "            severity = \"HIGH\"\n",
    "            details.append(f\"Missing/type-changed columns: {schema_drift}\")\n",
    "        elif schema_drift[\"new_columns\"]:\n",
    "            severity = \"MEDIUM\"\n",
    "            details.append(f\"New columns: {schema_drift['new_columns']}\")\n",
    "        if volume_drift:\n",
    "            severity = \"MEDIUM\"\n",
    "            details.append(f\"Volume drift: {volume_drift}\")\n",
    "        # 7. Save event if drift detected\n",
    "        if details:\n",
    "            save_drift_event(\n",
    "                spark,\n",
    "                table=table,\n",
    "                drift_type=\"schema/volume\",\n",
    "                severity=severity,\n",
    "                details=details,\n",
    "                baseline_json={\"schema\": schema_now, \"metrics\": metrics_now}\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e80631",
   "metadata": {},
   "source": [
    "## 4. Main Execution\n",
    "\n",
    "This section runs the drift monitoring process for all Silver tables and displays a summary of detected drift events. Results are also logged to the audit table for observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3672416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run drift monitoring for Silver tables\n",
    "run_silver_drift_monitoring(spark)\n",
    "\n",
    "# Display recent drift events from audit table (if exists)\n",
    "try:\n",
    "    if spark._jsparkSession.catalog().tableExists(AUDIT_TABLE):\n",
    "        display(spark.table(AUDIT_TABLE).orderBy(col(\"timestamp\").desc()).limit(10))\n",
    "    else:\n",
    "        print(\"Audit table does not exist yet.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying audit table: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f461d",
   "metadata": {},
   "source": [
    "## 5. Results and Audit Table\n",
    "\n",
    "This section displays the latest drift events detected for Silver tables. All events are logged in the Delta audit table `silver_drift_history` for traceability and observability. Review these results to monitor data health and take action if high-severity drift is detected.\n",
    "\n",
    "---\n",
    "\n",
    "> _For more details, see the monitoring/README.md and docs/data_dictionary.md files in the repository._"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
